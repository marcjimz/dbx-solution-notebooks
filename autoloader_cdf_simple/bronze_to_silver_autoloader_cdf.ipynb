{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "748cccbb-5cef-48b9-bc1c-0d2fa717f2f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Simple AutoLoader + Change Data Feed Demo\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. **Unity Catalog** setup\n",
    "2. **AutoLoader** for file ingestion (both batch and streaming)\n",
    "3. **Change Data Feed (CDF)** for incremental processing (both batch and streaming)\n",
    "4. **Feature Engineering** in the Silver layer\n",
    "\n",
    "## \uD83D\uDCCB Requirements\n",
    "- **Databricks Runtime**: 11.3 LTS or higher (for CDF support)\n",
    "- **Unity Catalog**: Enabled workspace (for managed tables)\n",
    "- **Cluster permissions**: CREATE TABLE, CREATE SCHEMA\n",
    "- **Storage**: Access to DBFS or cloud storage (ADLS Gen2, S3, or GCS)\n",
    "\n",
    "## \uD83C\uDFAF Batch vs Streaming\n",
    "**Important**: Both AutoLoader and CDF support batch AND streaming workloads!\n",
    "- **Batch**: Use `.trigger(once=True)` or regular DataFrame reads\n",
    "- **Streaming**: Use `.trigger(processingTime='X seconds')` for continuous processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d918f0ef-d8f8-4a80-a812-273690f0f14f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1️⃣ Setup and Configuration\n",
    "\n",
    "**\uD83D\uDCA1 Note**: We're using DBFS paths for data storage. In production, you can also use cloud storage paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91a947e3-6318-4f60-a2c8-a2685dba1a9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCCD Data path: dbfs:/FileStore/autoloader_demo/iris_landing/\n\uD83D\uDCCD Checkpoint base: dbfs:/FileStore/autoloader_demo/checkpoints/\n\uD83D\uDCCD Schema location base: dbfs:/FileStore/autoloader_demo/schemas/\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from delta.tables import DeltaTable\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "import time\n",
    "\n",
    "# Simple configuration using DBFS\n",
    "catalog = \"marcin_demo\"\n",
    "schema = \"iris_demo_cdf\"\n",
    "\n",
    "# DBFS paths for data and checkpoints\n",
    "data_path = \"dbfs:/FileStore/autoloader_demo/iris_landing/\"  \n",
    "checkpoint_base = \"dbfs:/FileStore/autoloader_demo/checkpoints/\"\n",
    "schema_location_base = \"dbfs:/FileStore/autoloader_demo/schemas/\"\n",
    "\n",
    "# For cloud storage, use paths like:\n",
    "# data_path = \"abfss://container@storage.dfs.core.windows.net/autoloader_demo/iris_landing/\"\n",
    "# data_path = \"s3a://bucket/autoloader_demo/iris_landing/\"\n",
    "\n",
    "print(f\"\uD83D\uDCCD Data path: {data_path}\")\n",
    "print(f\"\uD83D\uDCCD Checkpoint base: {checkpoint_base}\")\n",
    "print(f\"\uD83D\uDCCD Schema location base: {schema_location_base}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31c195f6-acad-4890-a3b0-23d6914cd222",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2️⃣ Create Unity Catalog Objects\n",
    "\n",
    "**\uD83D\uDD11 Key Concept**: Unity Catalog provides governance for Delta tables. CDF requires Unity Catalog managed tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28b1dc68-1925-466f-9699-5ec2ede6aa4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS silver_iris;\n",
    "DROP TABLE IF EXISTS bronze_iris;\n",
    "DROP TABLE IF EXISTS cdf_version_control;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcdecf02-1d25-4035-ac7e-b2581362251a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using: marcin_demo.iris_demo_cdf\n"
     ]
    }
   ],
   "source": [
    "# Create catalog and schema\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")\n",
    "spark.sql(f\"USE CATALOG {catalog}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema}\")\n",
    "spark.sql(f\"USE SCHEMA {schema}\")\n",
    "\n",
    "print(f\"✅ Using: {catalog}.{schema}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b85851b8-13aa-434e-b21b-a1ee74c0c6a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3️⃣ Generate IRIS Data in Chunks (Simulating File Arrivals)\n",
    "\n",
    "**\uD83C\uDFAF Real-world scenario**: Files typically arrive in batches (hourly, daily) or continuously in a landing zone.\n",
    "\n",
    "**Note**: We'll use Spark DataFrames to write CSV files directly to DBFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "850ff867-bf9b-4f04-b91a-6d6b33c143d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created DBFS directory: dbfs:/FileStore/autoloader_demo/iris_landing/\n✅ Created: dbfs:/FileStore/autoloader_demo/iris_landing/iris_batch_000.csv\n✅ Created: dbfs:/FileStore/autoloader_demo/iris_landing/iris_batch_001.csv\n✅ Created: dbfs:/FileStore/autoloader_demo/iris_landing/iris_batch_002.csv\n\n\uD83D\uDCC1 Files in landing zone:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/autoloader_demo/iris_landing/iris_batch_000.csv/</td><td>iris_batch_000.csv/</td><td>0</td><td>1753292945000</td></tr><tr><td>dbfs:/FileStore/autoloader_demo/iris_landing/iris_batch_001.csv/</td><td>iris_batch_001.csv/</td><td>0</td><td>1753292946000</td></tr><tr><td>dbfs:/FileStore/autoloader_demo/iris_landing/iris_batch_002.csv/</td><td>iris_batch_002.csv/</td><td>0</td><td>1753292947000</td></tr><tr><td>dbfs:/FileStore/autoloader_demo/iris_landing/iris_batch_003.csv/</td><td>iris_batch_003.csv/</td><td>0</td><td>1753292017000</td></tr><tr><td>dbfs:/FileStore/autoloader_demo/iris_landing/iris_batch_004.csv/</td><td>iris_batch_004.csv/</td><td>0</td><td>1753292018000</td></tr><tr><td>dbfs:/FileStore/autoloader_demo/iris_landing/iris_batch_005.csv/</td><td>iris_batch_005.csv/</td><td>0</td><td>1753292019000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/autoloader_demo/iris_landing/iris_batch_000.csv/",
         "iris_batch_000.csv/",
         0,
         1753292945000
        ],
        [
         "dbfs:/FileStore/autoloader_demo/iris_landing/iris_batch_001.csv/",
         "iris_batch_001.csv/",
         0,
         1753292946000
        ],
        [
         "dbfs:/FileStore/autoloader_demo/iris_landing/iris_batch_002.csv/",
         "iris_batch_002.csv/",
         0,
         1753292947000
        ],
        [
         "dbfs:/FileStore/autoloader_demo/iris_landing/iris_batch_003.csv/",
         "iris_batch_003.csv/",
         0,
         1753292017000
        ],
        [
         "dbfs:/FileStore/autoloader_demo/iris_landing/iris_batch_004.csv/",
         "iris_batch_004.csv/",
         0,
         1753292018000
        ],
        [
         "dbfs:/FileStore/autoloader_demo/iris_landing/iris_batch_005.csv/",
         "iris_batch_005.csv/",
         0,
         1753292019000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create data directory in DBFS using dbutils\n",
    "dbutils.fs.mkdirs(data_path)\n",
    "print(f\"✅ Created DBFS directory: {data_path}\")\n",
    "\n",
    "# Load IRIS dataset\n",
    "iris = load_iris()\n",
    "df_pandas = pd.DataFrame(data=iris.data, columns=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])\n",
    "df_pandas['species'] = iris.target_names[iris.target]\n",
    "\n",
    "# Generate 3 initial chunks using Spark\n",
    "for i in range(3):\n",
    "    # Create chunk with pandas\n",
    "    chunk = df_pandas.sample(n=50, replace=True).copy()\n",
    "    chunk['record_id'] = [f\"REC_{i:03d}_{j:04d}\" for j in range(len(chunk))]\n",
    "    chunk['batch_id'] = f\"batch_{i:03d}\"\n",
    "    chunk['timestamp'] = pd.Timestamp.now() + pd.Timedelta(hours=i)\n",
    "    \n",
    "    # Convert to Spark DataFrame and write to DBFS\n",
    "    spark_df = spark.createDataFrame(chunk)\n",
    "    file_path = f\"{data_path}iris_batch_{i:03d}.csv\"\n",
    "    \n",
    "    # Write as single CSV file (coalesce to 1 partition)\n",
    "    spark_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(file_path)\n",
    "    print(f\"✅ Created: {file_path}\")\n",
    "\n",
    "# List files in DBFS\n",
    "print(f\"\\n\uD83D\uDCC1 Files in landing zone:\")\n",
    "display(dbutils.fs.ls(data_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22e241a2-bcea-4cea-93b9-8551319928cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4️⃣ Create Bronze Table with AutoLoader\n",
    "\n",
    "### \uD83D\uDE80 AutoLoader Requirements:\n",
    "- **File format**: CSV, JSON, Parquet, Avro, Text, Binaryfile\n",
    "- **Schema location**: Required for schema inference and evolution (use DBFS path)\n",
    "- **Checkpoint**: Required for exactly-once processing (use DBFS path)\n",
    "\n",
    "**\uD83D\uDCA1 Best Practice**: Enable CDF on Bronze for downstream incremental processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6e0fdae-577e-4e03-b75c-9ec544757f1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Bronze table created with CDF enabled\n"
     ]
    }
   ],
   "source": [
    "# Create Bronze table with CDF enabled\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS bronze_iris (\n",
    "        sepal_length DOUBLE,\n",
    "        sepal_width DOUBLE,\n",
    "        petal_length DOUBLE,\n",
    "        petal_width DOUBLE,\n",
    "        species STRING,\n",
    "        record_id STRING,\n",
    "        batch_id STRING,\n",
    "        timestamp TIMESTAMP\n",
    "    )\n",
    "    USING DELTA\n",
    "    TBLPROPERTIES (\n",
    "        'delta.enableChangeDataFeed' = 'true',  -- Required for CDF\n",
    "        'delta.autoOptimize.optimizeWrite' = 'true'  -- Recommended for both batch and streaming\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ Bronze table created with CDF enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3754055-4cab-4e86-bc84-ec2f418dc405",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5️⃣ AutoLoader - BATCH Mode (One-time processing)\n",
    "\n",
    "**\uD83D\uDCCA Batch AutoLoader**: Perfect for scheduled jobs (hourly, daily) that process all new files at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bdf5584-5edd-49ee-8e55-e574ecdaadb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD04 Running AutoLoader in BATCH mode...\n✅ Batch AutoLoader completed!\n\uD83D\uDCCA Bronze records after batch load: 150\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>sepal_length</th><th>sepal_width</th><th>petal_length</th><th>petal_width</th><th>species</th><th>record_id</th><th>batch_id</th><th>timestamp</th><th>_rescued_data</th></tr></thead><tbody><tr><td>5.1</td><td>3.5</td><td>1.4</td><td>0.3</td><td>setosa</td><td>REC_000_0003</td><td>batch_000</td><td>2025-07-23T17:49:04.866Z</td><td>null</td></tr><tr><td>5.8</td><td>4.0</td><td>1.2</td><td>0.2</td><td>setosa</td><td>REC_000_0001</td><td>batch_000</td><td>2025-07-23T17:49:04.866Z</td><td>null</td></tr><tr><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>setosa</td><td>REC_000_0002</td><td>batch_000</td><td>2025-07-23T17:49:04.866Z</td><td>null</td></tr><tr><td>5.4</td><td>3.9</td><td>1.7</td><td>0.4</td><td>setosa</td><td>REC_000_0000</td><td>batch_000</td><td>2025-07-23T17:49:04.866Z</td><td>null</td></tr><tr><td>7.2</td><td>3.0</td><td>5.8</td><td>1.6</td><td>virginica</td><td>REC_000_0004</td><td>batch_000</td><td>2025-07-23T17:49:04.866Z</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         5.1,
         3.5,
         1.4,
         0.3,
         "setosa",
         "REC_000_0003",
         "batch_000",
         "2025-07-23T17:49:04.866Z",
         null
        ],
        [
         5.8,
         4.0,
         1.2,
         0.2,
         "setosa",
         "REC_000_0001",
         "batch_000",
         "2025-07-23T17:49:04.866Z",
         null
        ],
        [
         4.7,
         3.2,
         1.3,
         0.2,
         "setosa",
         "REC_000_0002",
         "batch_000",
         "2025-07-23T17:49:04.866Z",
         null
        ],
        [
         5.4,
         3.9,
         1.7,
         0.4,
         "setosa",
         "REC_000_0000",
         "batch_000",
         "2025-07-23T17:49:04.866Z",
         null
        ],
        [
         7.2,
         3.0,
         5.8,
         1.6,
         "virginica",
         "REC_000_0004",
         "batch_000",
         "2025-07-23T17:49:04.866Z",
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "sepal_length",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "sepal_width",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "petal_length",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "petal_width",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "species",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "record_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "batch_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "_rescued_data",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\uD83D\uDD04 Running AutoLoader in BATCH mode...\")\n",
    "\n",
    "# Define DBFS paths for schema and checkpoint\n",
    "bronze_schema_location = f\"{schema_location_base}bronze_batch/\"\n",
    "bronze_checkpoint_location = f\"{checkpoint_base}bronze_batch/\"\n",
    "\n",
    "batch_bronze_stream = (\n",
    "    spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"cloudFiles.schemaLocation\", bronze_schema_location)\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(data_path)\n",
    ")\n",
    "\n",
    "batch_query = (\n",
    "    batch_bronze_stream.writeStream\n",
    "        .format(\"delta\")\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"checkpointLocation\", bronze_checkpoint_location)\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .trigger(availableNow=True)  # Updated: recommended for 'batch' mode\n",
    "        .toTable(\"bronze_iris\")      # Use .toTable for table writes\n",
    ")\n",
    "\n",
    "batch_query.awaitTermination()\n",
    "\n",
    "print(f\"✅ Batch AutoLoader completed!\")\n",
    "print(f\"\uD83D\uDCCA Bronze records after batch load: {spark.table('bronze_iris').count()}\")\n",
    "display(spark.sql(\"SELECT * FROM bronze_iris ORDER BY timestamp LIMIT 5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "910b813a-df9e-4bf1-a55f-6d70e9316fdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6️⃣ Create Silver Table with Feature Engineering\n",
    "\n",
    "**\uD83C\uDFAF Silver Layer Purpose**: Cleansed, validated, and enriched data ready for analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a71ded4f-a20a-47d1-8151-5e3aeffaafbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Silver table created with CDF enabled\n"
     ]
    }
   ],
   "source": [
    "# Create Silver table with CDF enabled\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS silver_iris (\n",
    "        record_id STRING,\n",
    "        species STRING,\n",
    "        sepal_length DOUBLE,\n",
    "        sepal_width DOUBLE,\n",
    "        petal_length DOUBLE,\n",
    "        petal_width DOUBLE,\n",
    "        -- Feature engineering columns\n",
    "        sepal_area DOUBLE,\n",
    "        petal_area DOUBLE,\n",
    "        sepal_ratio DOUBLE,\n",
    "        petal_ratio DOUBLE,\n",
    "        size_category STRING,\n",
    "        -- Metadata for tracking\n",
    "        batch_id STRING,\n",
    "        original_timestamp TIMESTAMP,\n",
    "        silver_timestamp TIMESTAMP,\n",
    "        cdf_operation STRING,\n",
    "        processing_mode STRING  -- Track if batch or stream\n",
    "    )\n",
    "    USING DELTA\n",
    "    TBLPROPERTIES ('delta.enableChangeDataFeed' = 'true')\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ Silver table created with CDF enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ab331cd-fb8b-4f7a-b641-3f68e8339e20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7️⃣ Create Version Control Table\n",
    "\n",
    "### \uD83D\uDCCA Version Tracking for Incremental Processing\n",
    "**Purpose**: Track the last processed version from Bronze to avoid reprocessing data\n",
    "\n",
    "We'll create a control table to store processing metadata. **Streaming does not require this.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d329165b-1427-4f4b-acb2-e8a4342057f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD04 Processing Bronze to Silver using BATCH CDF with version tracking...\n✅ Version control table created\n✅ Version control initialized for bronze_iris\n"
     ]
    }
   ],
   "source": [
    "# BATCH CDF - Read changes from Bronze using version tracking\n",
    "print(\"\uD83D\uDD04 Processing Bronze to Silver using BATCH CDF with version tracking...\")\n",
    "\n",
    "# Create version control table\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS cdf_version_control (\n",
    "        table_name STRING,\n",
    "        last_processed_version LONG,\n",
    "        processing_timestamp TIMESTAMP,\n",
    "        records_processed LONG,\n",
    "        processing_status STRING,\n",
    "        processing_mode STRING,\n",
    "        error_message STRING\n",
    "    )\n",
    "    USING DELTA\n",
    "\"\"\")\n",
    "print(\"✅ Version control table created\")\n",
    "\n",
    "# Initialize version control for bronze_iris if not already present\n",
    "spark.sql(\"\"\"\n",
    "    MERGE INTO cdf_version_control AS target\n",
    "    USING (SELECT 'bronze_iris' AS table_name, 0 AS last_processed_version, \n",
    "                  current_timestamp() AS processing_timestamp, \n",
    "                  0 AS records_processed, \n",
    "                  'initialized' AS processing_status, \n",
    "                  'batch' AS processing_mode, \n",
    "                  NULL AS error_message) AS source\n",
    "    ON target.table_name = source.table_name\n",
    "    WHEN NOT MATCHED THEN\n",
    "    INSERT (table_name, last_processed_version, processing_timestamp, records_processed, processing_status, processing_mode, error_message)\n",
    "    VALUES (source.table_name, source.last_processed_version, source.processing_timestamp, source.records_processed, source.processing_status, source.processing_mode, source.error_message)\n",
    "\"\"\")\n",
    "print(\"✅ Version control initialized for bronze_iris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14060803-2813-4b7e-a22c-7c9cc23a818b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCCD Last processed version: 0\n"
     ]
    }
   ],
   "source": [
    "# Get last processed version from control table\n",
    "last_processed_version = spark.sql(\"\"\"\n",
    "    SELECT last_processed_version \n",
    "    FROM cdf_version_control \n",
    "    WHERE table_name = 'bronze_iris'\n",
    "\"\"\").collect()[0]['last_processed_version']\n",
    "\n",
    "print(f\"\uD83D\uDCCD Last processed version: {last_processed_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "059a295a-29e9-4939-8148-59522a6f8ecb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCCD Current Bronze table version: 2\n"
     ]
    }
   ],
   "source": [
    "# Get current Bronze table version\n",
    "current_bronze_version = spark.sql(\"DESCRIBE HISTORY bronze_iris\").select(\"version\").first()[0]\n",
    "print(f\"\uD83D\uDCCD Current Bronze table version: {current_bronze_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fe04d1c-ba78-4fc3-8ef7-9a26b57e9f16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_bronze_to_silver_cdf(last_processed_version, current_bronze_version):\n",
    "    # Check if there are new changes to process\n",
    "    if last_processed_version >= current_bronze_version:\n",
    "        print(\"✅ No new changes to process. Bronze table is up to date!\")\n",
    "    else:\n",
    "        print(f\"\uD83C\uDD95 Processing changes from version {last_processed_version + 1} to {current_bronze_version}\")\n",
    "        \n",
    "        try:\n",
    "            # Read only new changes since last processed version\n",
    "            bronze_changes_batch = (\n",
    "                spark.read\n",
    "                .format(\"delta\")\n",
    "                .option(\"readChangeFeed\", \"true\")\n",
    "                .option(\"startingVersion\", last_processed_version + 1)  # Start from next version\n",
    "                .option(\"endingVersion\", current_bronze_version)  # Up to current version\n",
    "                .table(\"bronze_iris\")\n",
    "            )\n",
    "            \n",
    "            print(f\"\uD83D\uDCCA CDF columns available: {[c for c in bronze_changes_batch.columns if c.startswith('_')]}\")\n",
    "            \n",
    "            # Filter for inserts and updates\n",
    "            filtered_changes = bronze_changes_batch.filter(\n",
    "                col(\"_change_type\").isin([\"insert\", \"update_postimage\"])\n",
    "            )\n",
    "            \n",
    "            changes_count = filtered_changes.count()\n",
    "            print(f\"\uD83D\uDCCA Changes to process: {changes_count}\")\n",
    "            \n",
    "            if changes_count > 0:\n",
    "                # Transform with feature engineering\n",
    "                silver_batch = (\n",
    "                    filtered_changes\n",
    "                    # Feature engineering\n",
    "                    .withColumn(\"sepal_area\", col(\"sepal_length\") * col(\"sepal_width\"))\n",
    "                    .withColumn(\"petal_area\", col(\"petal_length\") * col(\"petal_width\"))\n",
    "                    .withColumn(\"sepal_ratio\", col(\"sepal_length\") / col(\"sepal_width\"))\n",
    "                    .withColumn(\"petal_ratio\", col(\"petal_length\") / col(\"petal_width\"))\n",
    "                    .withColumn(\"size_category\", \n",
    "                        when(col(\"petal_area\") < 2, \"small\")\n",
    "                        .when(col(\"petal_area\") < 10, \"medium\")\n",
    "                        .otherwise(\"large\")\n",
    "                    )\n",
    "                    # Metadata\n",
    "                    .withColumn(\"original_timestamp\", col(\"timestamp\"))\n",
    "                    .withColumn(\"silver_timestamp\", current_timestamp())\n",
    "                    .withColumn(\"cdf_operation\", col(\"_change_type\"))\n",
    "                    .withColumn(\"processing_mode\", lit(\"batch\"))\n",
    "                    # Select final columns\n",
    "                    .select(\"record_id\", \"species\", \"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\",\n",
    "                            \"sepal_area\", \"petal_area\", \"sepal_ratio\", \"petal_ratio\", \"size_category\",\n",
    "                            \"batch_id\", \"original_timestamp\", \"silver_timestamp\", \"cdf_operation\", \"processing_mode\")\n",
    "                )\n",
    "                \n",
    "                # Write to Silver\n",
    "                silver_batch.write.format(\"delta\").mode(\"append\").saveAsTable(\"silver_iris\")\n",
    "                \n",
    "                # Update version control table with successful processing\n",
    "                spark.sql(f\"\"\"\n",
    "                    UPDATE cdf_version_control \n",
    "                    SET last_processed_version = {current_bronze_version},\n",
    "                        processing_timestamp = current_timestamp(),\n",
    "                        records_processed = {changes_count},\n",
    "                        processing_status = 'success',\n",
    "                        processing_mode = 'batch',\n",
    "                        error_message = null\n",
    "                    WHERE table_name = 'bronze_iris'\n",
    "                \"\"\")\n",
    "                \n",
    "                print(f\"✅ Batch CDF processing completed! Processed {changes_count} records\")\n",
    "                print(f\"\uD83D\uDCBE Updated control table - last processed version: {current_bronze_version}\")\n",
    "            else:\n",
    "                print(\"ℹ️ No insert or update changes found in the version range\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            # Log error in control table\n",
    "            error_msg = str(e).replace(\"'\", \"''\")  # Escape single quotes for SQL\n",
    "            spark.sql(f\"\"\"\n",
    "                UPDATE cdf_version_control \n",
    "                SET processing_timestamp = current_timestamp(),\n",
    "                    processing_status = 'failed',\n",
    "                    error_message = '{error_msg}'\n",
    "                WHERE table_name = 'bronze_iris'\n",
    "            \"\"\")\n",
    "            print(f\"❌ Error during processing: {e}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c24b7a4-c445-4f87-9b26-99113bc26f6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83C\uDD95 Processing changes from version 1 to 2\n\uD83D\uDCCA CDF columns available: ['_rescued_data', '_change_type', '_commit_version', '_commit_timestamp']\n\uD83D\uDCCA Changes to process: 150\n✅ Batch CDF processing completed! Processed 150 records\n\uD83D\uDCBE Updated control table - last processed version: 2\n\n\uD83D\uDCCA Sample Silver records from this batch:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>record_id</th><th>species</th><th>sepal_length</th><th>sepal_width</th><th>petal_length</th><th>petal_width</th><th>sepal_area</th><th>petal_area</th><th>sepal_ratio</th><th>petal_ratio</th><th>size_category</th><th>batch_id</th><th>original_timestamp</th><th>silver_timestamp</th><th>cdf_operation</th><th>processing_mode</th></tr></thead><tbody><tr><td>REC_002_0000</td><td>virginica</td><td>7.7</td><td>3.0</td><td>6.1</td><td>2.3</td><td>23.1</td><td>14.029999999999998</td><td>2.566666666666667</td><td>2.6521739130434785</td><td>large</td><td>batch_002</td><td>2025-07-23T19:49:06.871Z</td><td>2025-07-23T17:54:30.276Z</td><td>insert</td><td>batch</td></tr><tr><td>REC_002_0001</td><td>setosa</td><td>5.1</td><td>3.8</td><td>1.9</td><td>0.4</td><td>19.38</td><td>0.76</td><td>1.3421052631578947</td><td>4.749999999999999</td><td>small</td><td>batch_002</td><td>2025-07-23T19:49:06.871Z</td><td>2025-07-23T17:54:30.276Z</td><td>insert</td><td>batch</td></tr><tr><td>REC_002_0003</td><td>virginica</td><td>7.4</td><td>2.8</td><td>6.1</td><td>1.9</td><td>20.72</td><td>11.589999999999998</td><td>2.6428571428571432</td><td>3.2105263157894735</td><td>large</td><td>batch_002</td><td>2025-07-23T19:49:06.871Z</td><td>2025-07-23T17:54:30.276Z</td><td>insert</td><td>batch</td></tr><tr><td>REC_002_0002</td><td>setosa</td><td>5.5</td><td>4.2</td><td>1.4</td><td>0.2</td><td>23.1</td><td>0.27999999999999997</td><td>1.3095238095238095</td><td>6.999999999999999</td><td>small</td><td>batch_002</td><td>2025-07-23T19:49:06.871Z</td><td>2025-07-23T17:54:30.276Z</td><td>insert</td><td>batch</td></tr><tr><td>REC_002_0004</td><td>virginica</td><td>6.5</td><td>3.0</td><td>5.8</td><td>2.2</td><td>19.5</td><td>12.76</td><td>2.1666666666666665</td><td>2.6363636363636362</td><td>large</td><td>batch_002</td><td>2025-07-23T19:49:06.871Z</td><td>2025-07-23T17:54:30.276Z</td><td>insert</td><td>batch</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "REC_002_0000",
         "virginica",
         7.7,
         3.0,
         6.1,
         2.3,
         23.1,
         14.029999999999998,
         2.566666666666667,
         2.6521739130434785,
         "large",
         "batch_002",
         "2025-07-23T19:49:06.871Z",
         "2025-07-23T17:54:30.276Z",
         "insert",
         "batch"
        ],
        [
         "REC_002_0001",
         "setosa",
         5.1,
         3.8,
         1.9,
         0.4,
         19.38,
         0.76,
         1.3421052631578947,
         4.749999999999999,
         "small",
         "batch_002",
         "2025-07-23T19:49:06.871Z",
         "2025-07-23T17:54:30.276Z",
         "insert",
         "batch"
        ],
        [
         "REC_002_0003",
         "virginica",
         7.4,
         2.8,
         6.1,
         1.9,
         20.72,
         11.589999999999998,
         2.6428571428571432,
         3.2105263157894735,
         "large",
         "batch_002",
         "2025-07-23T19:49:06.871Z",
         "2025-07-23T17:54:30.276Z",
         "insert",
         "batch"
        ],
        [
         "REC_002_0002",
         "setosa",
         5.5,
         4.2,
         1.4,
         0.2,
         23.1,
         0.27999999999999997,
         1.3095238095238095,
         6.999999999999999,
         "small",
         "batch_002",
         "2025-07-23T19:49:06.871Z",
         "2025-07-23T17:54:30.276Z",
         "insert",
         "batch"
        ],
        [
         "REC_002_0004",
         "virginica",
         6.5,
         3.0,
         5.8,
         2.2,
         19.5,
         12.76,
         2.1666666666666665,
         2.6363636363636362,
         "large",
         "batch_002",
         "2025-07-23T19:49:06.871Z",
         "2025-07-23T17:54:30.276Z",
         "insert",
         "batch"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "record_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "species",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sepal_length",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "sepal_width",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "petal_length",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "petal_width",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "sepal_area",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "petal_area",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "sepal_ratio",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "petal_ratio",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "size_category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "batch_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "original_timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "silver_timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "cdf_operation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "processing_mode",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "process_bronze_to_silver_cdf(last_processed_version, current_bronze_version)\n",
    "\n",
    "# Display sample of processed records\n",
    "print(\"\\n\uD83D\uDCCA Sample Silver records from this batch:\")\n",
    "display(spark.sql(\"\"\"\n",
    "    SELECT * FROM silver_iris \n",
    "    WHERE processing_mode = 'batch' \n",
    "    ORDER BY silver_timestamp DESC \n",
    "    LIMIT 5\n",
    "    \"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd4458ef-d195-4089-9ec4-a32b41284392",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 8️⃣ Generate More Files (Simulating next batch arrival)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32dd8aa8-d0bd-4ae1-8275-5583ba817104",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCC2 Generating new batch of files...\n✅ Created: dbfs:/FileStore/autoloader_demo/iris_landing/iris_batch_003.csv\n✅ Created: dbfs:/FileStore/autoloader_demo/iris_landing/iris_batch_004.csv\n✅ Created: dbfs:/FileStore/autoloader_demo/iris_landing/iris_batch_005.csv\n\n\uD83D\uDCC1 New files ready for next batch run!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/autoloader_demo/iris_landing/iris_batch_000.csv/</td><td>iris_batch_000.csv/</td><td>0</td><td>1753292945000</td></tr><tr><td>dbfs:/FileStore/autoloader_demo/iris_landing/iris_batch_001.csv/</td><td>iris_batch_001.csv/</td><td>0</td><td>1753292946000</td></tr><tr><td>dbfs:/FileStore/autoloader_demo/iris_landing/iris_batch_002.csv/</td><td>iris_batch_002.csv/</td><td>0</td><td>1753292947000</td></tr><tr><td>dbfs:/FileStore/autoloader_demo/iris_landing/iris_batch_003.csv/</td><td>iris_batch_003.csv/</td><td>0</td><td>1753293307000</td></tr><tr><td>dbfs:/FileStore/autoloader_demo/iris_landing/iris_batch_004.csv/</td><td>iris_batch_004.csv/</td><td>0</td><td>1753293308000</td></tr><tr><td>dbfs:/FileStore/autoloader_demo/iris_landing/iris_batch_005.csv/</td><td>iris_batch_005.csv/</td><td>0</td><td>1753293309000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/autoloader_demo/iris_landing/iris_batch_000.csv/",
         "iris_batch_000.csv/",
         0,
         1753292945000
        ],
        [
         "dbfs:/FileStore/autoloader_demo/iris_landing/iris_batch_001.csv/",
         "iris_batch_001.csv/",
         0,
         1753292946000
        ],
        [
         "dbfs:/FileStore/autoloader_demo/iris_landing/iris_batch_002.csv/",
         "iris_batch_002.csv/",
         0,
         1753292947000
        ],
        [
         "dbfs:/FileStore/autoloader_demo/iris_landing/iris_batch_003.csv/",
         "iris_batch_003.csv/",
         0,
         1753293307000
        ],
        [
         "dbfs:/FileStore/autoloader_demo/iris_landing/iris_batch_004.csv/",
         "iris_batch_004.csv/",
         0,
         1753293308000
        ],
        [
         "dbfs:/FileStore/autoloader_demo/iris_landing/iris_batch_005.csv/",
         "iris_batch_005.csv/",
         0,
         1753293309000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate more files to show batch processing\n",
    "print(\"\uD83D\uDCC2 Generating new batch of files...\")\n",
    "for i in range(3, 6):\n",
    "    # Create chunk with pandas\n",
    "    chunk = df_pandas.sample(n=30, replace=True).copy()\n",
    "    chunk['record_id'] = [f\"REC_{i:03d}_{j:04d}\" for j in range(len(chunk))]\n",
    "    chunk['batch_id'] = f\"batch_{i:03d}\"\n",
    "    chunk['timestamp'] = pd.Timestamp.now()\n",
    "    \n",
    "    # Convert to Spark DataFrame and write to DBFS\n",
    "    spark_df = spark.createDataFrame(chunk)\n",
    "    file_path = f\"{data_path}iris_batch_{i:03d}.csv\"\n",
    "    \n",
    "    # Write as single CSV file\n",
    "    spark_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(file_path)\n",
    "    print(f\"✅ Created: {file_path}\")\n",
    "\n",
    "print(\"\\n\uD83D\uDCC1 New files ready for next batch run!\")\n",
    "display(dbutils.fs.ls(data_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f85ca856-2f25-4f8d-8c81-2b0d42437533",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 9️⃣ AutoLoader - BATCH Mode Again (Process only new files)\n",
    "\n",
    "**\uD83D\uDCA1 Key Point**: AutoLoader tracks processed files in its checkpoint, so it only processes NEW files in each batch run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5874d495-29f1-42f6-9a18-74ecdac435d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCCA Bronze records before: 150\n\n\uD83D\uDD04 Running AutoLoader BATCH again (only new files)...\n\uD83D\uDD04 Running AutoLoader in BATCH mode...\n\n✅ Batch completed!\n\uD83D\uDCCA Bronze records after: 240\n\uD83C\uDD95 New records added: 90\n\n\uD83D\uDCA1 AutoLoader only processed the NEW files!\n"
     ]
    }
   ],
   "source": [
    "# Get current Bronze count before processing\n",
    "bronze_count_before = spark.table('bronze_iris').count()\n",
    "print(f\"\uD83D\uDCCA Bronze records before: {bronze_count_before}\")\n",
    "\n",
    "# Run AutoLoader batch again - it will only process NEW files\n",
    "print(\"\\n\uD83D\uDD04 Running AutoLoader BATCH again (only new files)...\")\n",
    "\n",
    "print(\"\uD83D\uDD04 Running AutoLoader in BATCH mode...\")\n",
    "\n",
    "batch_bronze_stream_2 = (\n",
    "    spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"cloudFiles.schemaLocation\", bronze_schema_location)\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(data_path)\n",
    ")\n",
    "\n",
    "batch_query_2 = (\n",
    "    batch_bronze_stream.writeStream\n",
    "        .format(\"delta\")\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"checkpointLocation\", bronze_checkpoint_location)\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .trigger(availableNow=True)  # Updated: recommended for 'batch' mode\n",
    "        .toTable(\"bronze_iris\")      # Use .toTable for table writes\n",
    ")\n",
    "\n",
    "batch_query_2.awaitTermination()\n",
    "bronze_count_after = spark.table('bronze_iris').count()\n",
    "print(f\"\\n✅ Batch completed!\")\n",
    "print(f\"\uD83D\uDCCA Bronze records after: {bronze_count_after}\")\n",
    "print(f\"\uD83C\uDD95 New records added: {bronze_count_after - bronze_count_before}\")\n",
    "print(\"\\n\uD83D\uDCA1 AutoLoader only processed the NEW files!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5812f54-47cf-4115-aa06-b3b0b278a13b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83D\uDD1F BATCH Mode CDF Process Again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c47e763-f0ef-4022-82e3-3507f1e18027",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCCD Last processed version: 2\n\uD83D\uDCCD Current Bronze table version: 3\n"
     ]
    }
   ],
   "source": [
    "last_processed_version = spark.sql(\"\"\"\n",
    "    SELECT last_processed_version \n",
    "    FROM cdf_version_control \n",
    "    WHERE table_name = 'bronze_iris'\n",
    "\"\"\").collect()[0]['last_processed_version']\n",
    "\n",
    "print(f\"\uD83D\uDCCD Last processed version: {last_processed_version}\")\n",
    "\n",
    "# Get current Bronze table version\n",
    "current_bronze_version = spark.sql(\"DESCRIBE HISTORY bronze_iris\").select(\"version\").first()[0]\n",
    "print(f\"\uD83D\uDCCD Current Bronze table version: {current_bronze_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45f1d4ff-5a5e-4059-a0cc-813bdff63c62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83C\uDD95 Processing changes from version 3 to 3\n\uD83D\uDCCA CDF columns available: ['_rescued_data', '_change_type', '_commit_version', '_commit_timestamp']\n\uD83D\uDCCA Changes to process: 90\n✅ Batch CDF processing completed! Processed 90 records\n\uD83D\uDCBE Updated control table - last processed version: 3\n\n\uD83D\uDCCA Sample Silver records from this batch:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>record_id</th><th>species</th><th>sepal_length</th><th>sepal_width</th><th>petal_length</th><th>petal_width</th><th>sepal_area</th><th>petal_area</th><th>sepal_ratio</th><th>petal_ratio</th><th>size_category</th><th>batch_id</th><th>original_timestamp</th><th>silver_timestamp</th><th>cdf_operation</th><th>processing_mode</th></tr></thead><tbody><tr><td>REC_005_0000</td><td>versicolor</td><td>5.5</td><td>2.6</td><td>4.4</td><td>1.2</td><td>14.3</td><td>5.28</td><td>2.1153846153846154</td><td>3.666666666666667</td><td>medium</td><td>batch_005</td><td>2025-07-23T17:55:08.186Z</td><td>2025-07-23T17:55:39.981Z</td><td>insert</td><td>batch</td></tr><tr><td>REC_005_0001</td><td>virginica</td><td>6.9</td><td>3.1</td><td>5.1</td><td>2.3</td><td>21.39</td><td>11.729999999999999</td><td>2.2258064516129035</td><td>2.217391304347826</td><td>large</td><td>batch_005</td><td>2025-07-23T17:55:08.186Z</td><td>2025-07-23T17:55:39.981Z</td><td>insert</td><td>batch</td></tr><tr><td>REC_005_0003</td><td>versicolor</td><td>6.0</td><td>3.4</td><td>4.5</td><td>1.6</td><td>20.4</td><td>7.2</td><td>1.7647058823529411</td><td>2.8125</td><td>medium</td><td>batch_005</td><td>2025-07-23T17:55:08.186Z</td><td>2025-07-23T17:55:39.981Z</td><td>insert</td><td>batch</td></tr><tr><td>REC_005_0002</td><td>setosa</td><td>4.8</td><td>3.4</td><td>1.6</td><td>0.2</td><td>16.32</td><td>0.32000000000000006</td><td>1.411764705882353</td><td>8.0</td><td>small</td><td>batch_005</td><td>2025-07-23T17:55:08.186Z</td><td>2025-07-23T17:55:39.981Z</td><td>insert</td><td>batch</td></tr><tr><td>REC_005_0004</td><td>virginica</td><td>6.9</td><td>3.1</td><td>5.4</td><td>2.1</td><td>21.39</td><td>11.340000000000002</td><td>2.2258064516129035</td><td>2.5714285714285716</td><td>large</td><td>batch_005</td><td>2025-07-23T17:55:08.186Z</td><td>2025-07-23T17:55:39.981Z</td><td>insert</td><td>batch</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "REC_005_0000",
         "versicolor",
         5.5,
         2.6,
         4.4,
         1.2,
         14.3,
         5.28,
         2.1153846153846154,
         3.666666666666667,
         "medium",
         "batch_005",
         "2025-07-23T17:55:08.186Z",
         "2025-07-23T17:55:39.981Z",
         "insert",
         "batch"
        ],
        [
         "REC_005_0001",
         "virginica",
         6.9,
         3.1,
         5.1,
         2.3,
         21.39,
         11.729999999999999,
         2.2258064516129035,
         2.217391304347826,
         "large",
         "batch_005",
         "2025-07-23T17:55:08.186Z",
         "2025-07-23T17:55:39.981Z",
         "insert",
         "batch"
        ],
        [
         "REC_005_0003",
         "versicolor",
         6.0,
         3.4,
         4.5,
         1.6,
         20.4,
         7.2,
         1.7647058823529411,
         2.8125,
         "medium",
         "batch_005",
         "2025-07-23T17:55:08.186Z",
         "2025-07-23T17:55:39.981Z",
         "insert",
         "batch"
        ],
        [
         "REC_005_0002",
         "setosa",
         4.8,
         3.4,
         1.6,
         0.2,
         16.32,
         0.32000000000000006,
         1.411764705882353,
         8.0,
         "small",
         "batch_005",
         "2025-07-23T17:55:08.186Z",
         "2025-07-23T17:55:39.981Z",
         "insert",
         "batch"
        ],
        [
         "REC_005_0004",
         "virginica",
         6.9,
         3.1,
         5.4,
         2.1,
         21.39,
         11.340000000000002,
         2.2258064516129035,
         2.5714285714285716,
         "large",
         "batch_005",
         "2025-07-23T17:55:08.186Z",
         "2025-07-23T17:55:39.981Z",
         "insert",
         "batch"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "record_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "species",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sepal_length",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "sepal_width",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "petal_length",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "petal_width",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "sepal_area",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "petal_area",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "sepal_ratio",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "petal_ratio",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "size_category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "batch_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "original_timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "silver_timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "cdf_operation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "processing_mode",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "process_bronze_to_silver_cdf(last_processed_version, current_bronze_version)\n",
    "\n",
    "# Display sample of processed records\n",
    "print(\"\\n\uD83D\uDCCA Sample Silver records from this batch:\")\n",
    "display(spark.sql(\"\"\"\n",
    "    SELECT * FROM silver_iris \n",
    "    WHERE processing_mode = 'batch' \n",
    "    ORDER BY silver_timestamp DESC \n",
    "    LIMIT 5\n",
    "    \"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36f2f9b6-c753-4299-8f05-2ff1e18241f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Close it out - no more new processing required! \n",
    "\n",
    "We check the versions once more, and confirm there's no more processing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "018211f8-3fa5-4768-a2e7-fa9ec569584a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCCD Last processed version: 3\n\uD83D\uDCCD Current Bronze table version: 3\n✅ No new changes to process. Bronze table is up to date!\n"
     ]
    }
   ],
   "source": [
    "last_processed_version = spark.sql(\"\"\"\n",
    "    SELECT last_processed_version \n",
    "    FROM cdf_version_control \n",
    "    WHERE table_name = 'bronze_iris'\n",
    "\"\"\").collect()[0]['last_processed_version']\n",
    "\n",
    "print(f\"\uD83D\uDCCD Last processed version: {last_processed_version}\")\n",
    "\n",
    "# Get current Bronze table version\n",
    "current_bronze_version = spark.sql(\"DESCRIBE HISTORY bronze_iris\").select(\"version\").first()[0]\n",
    "print(f\"\uD83D\uDCCD Current Bronze table version: {current_bronze_version}\")\n",
    "\n",
    "process_bronze_to_silver_cdf(last_processed_version, current_bronze_version)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8418158362761080,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "bronze_to_silver_autoloader_cdf _final",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}